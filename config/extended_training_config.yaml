enhanced_params:
  antenna_efficiency: 0.75
  implementation_loss_dB: 3.0
environment:
  humidity: 50.0
  pointing_error_std: 0.005
  pressure: 101.3
  rician_k_factor: 8.0
  temperature: 20.0
  turbulence_strength: 1e-14
evaluation:
  eval_episodes: 20  # Increased for better evaluation
  eval_frequency: 100  # Evaluate every 100 episodes
  save_model_frequency: 100  # Save models every 100 episodes
exploration:
  epsilon_decay: 0.995  # Slower decay for longer training
  epsilon_end: 0.01
  epsilon_start: 1.0
mobility:
  direction_change_prob: 0.1
  max_speed: 5.0
  min_speed: 0.5
network:
  action_dim: 3
  activation: relu
  hidden_layers:
  - 128
  - 128
  state_dim: 8
oam:
  beam_width: 0.03
  max_mode: 6
  min_mode: 1
  mode_spacing: 1
replay_buffer:
  capacity: 100000  # Increased for longer training
  min_samples_to_learn: 1000
reward:
  handover_penalty: 0.5
  outage_penalty: 5.0
  sinr_threshold: -5.0
  throughput_factor: 5.0
rl_env:
  handover_penalty: 0.2
  outage_penalty: 1.0
  sinr_threshold: -5.0
  throughput_factor: 1.0
stable_reward:
  reward_max: 10.0
  reward_min: -10.0
  reward_scale: 2.0
  sinr_scaling_factor: 0.1
  smoothing_factor: 0.7
  window_size: 10
system:
  bandwidth: 400e6
  frequency: 28.0e9
  noise_figure_dB: 8.0
  noise_temp: 290.0
  tx_power_dBm: 30.0
training:
  batch_size: 128
  episodes: 1000  # Extended to 1000 episodes
  epsilon_decay: 0.995  # Slower decay
  epsilon_end: 0.01
  epsilon_start: 1.0
  gamma: 0.99
  learning_rate: 0.0001
  max_steps_per_episode: 500
  memory_size: 100000  # Increased buffer size
  num_episodes: 1000  # Extended training
  target_update_freq: 20  # Update target network less frequently
  target_update_interval: 20 